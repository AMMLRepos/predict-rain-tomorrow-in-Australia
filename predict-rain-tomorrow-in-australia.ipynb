{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Title\nTo predict if it will rain tomorrow in Australia using Decision Tree Classifier ","metadata":{}},{"cell_type":"markdown","source":"## Overview\nPredicting weather for rain, sunshine, cold, etc. has been the most usual application of forcasting and prediction in Machine Learning. We will use Australia's weather and rain dataset with data of almost 10 years to predict if it will rain tomorrow. \n\n## Objective \nTo apply Data science, data analysis and machine learning on a sample problem for classification (Yes/No)\n\n## Dataset \nData is openly available on Kaggle on this link - https://www.kaggle.com/jsphyg/weather-dataset-rattle-package","metadata":{}},{"cell_type":"markdown","source":"## Steps\nWe will follow steps given below - \n* Install and import required libraries \n* Download dataset and import it in pandas \n* Perform basic descriptive analytics on the data \n* Visualize data for patterns and identify critical features impacting the decision\n* Prepare data for training - data cleaning, data normalization, data encoding, train/test split, etc. \n* Train data using Decision tree Classifier\n* Evaluate and improve model performance \n* Packaging model ","metadata":{}},{"cell_type":"markdown","source":"# Import required libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\n#Set basic configuration for pandas, matplotlib and seaborn\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 150)\nsns.set_style(\"darkgrid\")\n\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:25.525939Z","iopub.execute_input":"2021-12-20T11:55:25.526422Z","iopub.status.idle":"2021-12-20T11:55:25.532647Z","shell.execute_reply.started":"2021-12-20T11:55:25.526373Z","shell.execute_reply":"2021-12-20T11:55:25.531855Z"},"trusted":true},"execution_count":638,"outputs":[]},{"cell_type":"markdown","source":"# Downloading/Import data ","metadata":{}},{"cell_type":"code","source":"raw_df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")\nraw_df","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:25.544387Z","iopub.execute_input":"2021-12-20T11:55:25.544655Z","iopub.status.idle":"2021-12-20T11:55:25.993268Z","shell.execute_reply.started":"2021-12-20T11:55:25.544627Z","shell.execute_reply":"2021-12-20T11:55:25.992368Z"},"trusted":true},"execution_count":639,"outputs":[]},{"cell_type":"code","source":"raw_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:25.994820Z","iopub.execute_input":"2021-12-20T11:55:25.995090Z","iopub.status.idle":"2021-12-20T11:55:26.121832Z","shell.execute_reply.started":"2021-12-20T11:55:25.995059Z","shell.execute_reply":"2021-12-20T11:55:26.120779Z"},"trusted":true},"execution_count":640,"outputs":[]},{"cell_type":"markdown","source":"From above output we can see that there are many missing values in the dataset. We need to handle missing data in various steps to ensure that our ML model training does not fail. \nOn priority, let us drop all null values from RainTomorrow column. We need to do this as RainTomorrow is the target column and must have a value for model to get trained.","metadata":{}},{"cell_type":"code","source":"#drop all null values from dataframe[RainTomorrow]\nraw_df.dropna(subset = [\"RainTomorrow\"], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:26.123289Z","iopub.execute_input":"2021-12-20T11:55:26.123684Z","iopub.status.idle":"2021-12-20T11:55:26.163289Z","shell.execute_reply.started":"2021-12-20T11:55:26.123638Z","shell.execute_reply":"2021-12-20T11:55:26.162264Z"},"trusted":true},"execution_count":641,"outputs":[]},{"cell_type":"code","source":"raw_df.describe().T","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:26.165355Z","iopub.execute_input":"2021-12-20T11:55:26.165584Z","iopub.status.idle":"2021-12-20T11:55:26.303924Z","shell.execute_reply.started":"2021-12-20T11:55:26.165557Z","shell.execute_reply":"2021-12-20T11:55:26.303170Z"},"trusted":true},"execution_count":642,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory data analysis","metadata":{}},{"cell_type":"markdown","source":"Let us try heatmap of correlation between all features in the dataset","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (20,10))\nsns.heatmap(raw_df.corr(), annot = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:26.305083Z","iopub.execute_input":"2021-12-20T11:55:26.305298Z","iopub.status.idle":"2021-12-20T11:55:28.575960Z","shell.execute_reply.started":"2021-12-20T11:55:26.305270Z","shell.execute_reply":"2021-12-20T11:55:28.575040Z"},"trusted":true},"execution_count":643,"outputs":[]},{"cell_type":"markdown","source":"From above heatmap of correlation, we can see that there are a few features which are impacting other and can be termed as positively correlated","metadata":{}},{"cell_type":"code","source":"sns.boxplot(x = \"MinTemp\", y = \"RainTomorrow\", data = raw_df, dodge = True);","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:28.577199Z","iopub.execute_input":"2021-12-20T11:55:28.577419Z","iopub.status.idle":"2021-12-20T11:55:28.953044Z","shell.execute_reply.started":"2021-12-20T11:55:28.577392Z","shell.execute_reply":"2021-12-20T11:55:28.952075Z"},"trusted":true},"execution_count":644,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = \"MaxTemp\", y = \"RainTomorrow\", data = raw_df, dodge = True);","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:28.954604Z","iopub.execute_input":"2021-12-20T11:55:28.955156Z","iopub.status.idle":"2021-12-20T11:55:29.341014Z","shell.execute_reply.started":"2021-12-20T11:55:28.955088Z","shell.execute_reply":"2021-12-20T11:55:29.340130Z"},"trusted":true},"execution_count":645,"outputs":[]},{"cell_type":"code","source":"sns.boxplot(x = \"Cloud9am\", y = \"RainTomorrow\", data = raw_df, dodge = True);","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.342206Z","iopub.execute_input":"2021-12-20T11:55:29.342429Z","iopub.status.idle":"2021-12-20T11:55:29.712961Z","shell.execute_reply.started":"2021-12-20T11:55:29.342402Z","shell.execute_reply":"2021-12-20T11:55:29.712038Z"},"trusted":true},"execution_count":646,"outputs":[]},{"cell_type":"markdown","source":"# Preparing data for training","metadata":{}},{"cell_type":"markdown","source":"## Dealing with missing values\nMissing values are to be fixed before being fed to a model. Also, they must be dealt properly to ensure that we dont tamper the data with nonsense entries. \nLet us get the count of missing values in the dataset for each column","metadata":{}},{"cell_type":"code","source":"raw_df.isna().sum().sort_values(ascending = False)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.714536Z","iopub.execute_input":"2021-12-20T11:55:29.714907Z","iopub.status.idle":"2021-12-20T11:55:29.836693Z","shell.execute_reply.started":"2021-12-20T11:55:29.714865Z","shell.execute_reply":"2021-12-20T11:55:29.836107Z"},"trusted":true},"execution_count":647,"outputs":[]},{"cell_type":"markdown","source":"As we can see there are many missing values. For example Sunshine has 67,816 entries missing which is almost half of the total records we have. imiliarly Evaporation, Cloud3pm and Cloud9am have over 50,000 missing values. \nWe cannnot remove this many number of records as we will loose almost half of our dataset and this will impact our model training as we will not have enough data to train on. We might have to deal seperately with different category of columns and so let us seperate numeric and categorical/string columns ","metadata":{}},{"cell_type":"code","source":"#Fetching numeric columns from the dataset\nnumeric_cols = raw_df.select_dtypes(include = np.number).columns.to_list()\nprint(numeric_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.839205Z","iopub.execute_input":"2021-12-20T11:55:29.839545Z","iopub.status.idle":"2021-12-20T11:55:29.849426Z","shell.execute_reply.started":"2021-12-20T11:55:29.839515Z","shell.execute_reply":"2021-12-20T11:55:29.848781Z"},"trusted":true},"execution_count":648,"outputs":[]},{"cell_type":"code","source":"#Fetching categorical or string columns from the dataset\ncategorical_cols = raw_df.select_dtypes('object').columns.to_list()\nprint(categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.850448Z","iopub.execute_input":"2021-12-20T11:55:29.850767Z","iopub.status.idle":"2021-12-20T11:55:29.875704Z","shell.execute_reply.started":"2021-12-20T11:55:29.850739Z","shell.execute_reply":"2021-12-20T11:55:29.874788Z"},"trusted":true},"execution_count":649,"outputs":[]},{"cell_type":"markdown","source":"### Dealing with missing values in numercial columns\nOne can use median, mode and mean to replace missing numerical values. For simplicity, we will use mean to replace missing values","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy = \"mean\").fit(raw_df[numeric_cols])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.876950Z","iopub.execute_input":"2021-12-20T11:55:29.877213Z","iopub.status.idle":"2021-12-20T11:55:29.908768Z","shell.execute_reply.started":"2021-12-20T11:55:29.877181Z","shell.execute_reply":"2021-12-20T11:55:29.908151Z"},"trusted":true},"execution_count":650,"outputs":[]},{"cell_type":"code","source":"#apply the mean imputing strategy on numerical columns of raw_df\nraw_df[numeric_cols] = imputer.transform(raw_df[numeric_cols])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.910226Z","iopub.execute_input":"2021-12-20T11:55:29.910707Z","iopub.status.idle":"2021-12-20T11:55:29.952986Z","shell.execute_reply.started":"2021-12-20T11:55:29.910665Z","shell.execute_reply":"2021-12-20T11:55:29.952153Z"},"trusted":true},"execution_count":651,"outputs":[]},{"cell_type":"markdown","source":"Let us now check if we have missing values","metadata":{}},{"cell_type":"code","source":"raw_df[numeric_cols].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.954386Z","iopub.execute_input":"2021-12-20T11:55:29.954691Z","iopub.status.idle":"2021-12-20T11:55:29.973179Z","shell.execute_reply.started":"2021-12-20T11:55:29.954652Z","shell.execute_reply":"2021-12-20T11:55:29.972231Z"},"trusted":true},"execution_count":652,"outputs":[]},{"cell_type":"markdown","source":"### Dealing with missing values in Categorical columns","metadata":{}},{"cell_type":"code","source":"raw_df[categorical_cols].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:29.974682Z","iopub.execute_input":"2021-12-20T11:55:29.975195Z","iopub.status.idle":"2021-12-20T11:55:30.100898Z","shell.execute_reply.started":"2021-12-20T11:55:29.975159Z","shell.execute_reply":"2021-12-20T11:55:30.099979Z"},"trusted":true},"execution_count":653,"outputs":[]},{"cell_type":"markdown","source":"Let us replace all null/missing values with \"Unknown\" which will become another category ","metadata":{}},{"cell_type":"code","source":"raw_df[categorical_cols] = raw_df[categorical_cols].fillna(\"unknown\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.102017Z","iopub.execute_input":"2021-12-20T11:55:30.102273Z","iopub.status.idle":"2021-12-20T11:55:30.237735Z","shell.execute_reply.started":"2021-12-20T11:55:30.102246Z","shell.execute_reply":"2021-12-20T11:55:30.236567Z"},"trusted":true},"execution_count":654,"outputs":[]},{"cell_type":"code","source":"raw_df[categorical_cols].isna().sum()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.239722Z","iopub.execute_input":"2021-12-20T11:55:30.240047Z","iopub.status.idle":"2021-12-20T11:55:30.365918Z","shell.execute_reply.started":"2021-12-20T11:55:30.240008Z","shell.execute_reply":"2021-12-20T11:55:30.365101Z"},"trusted":true},"execution_count":655,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering \nday and month might have a significant impact on the prediction. For example in India, it is highly likely to rain in monsoon than in winters. Let break date into day, month and year and then drop date column from the dataset ","metadata":{}},{"cell_type":"code","source":"raw_df[\"year\"] = pd.to_datetime(raw_df.Date).dt.year;\nraw_df[\"month\"] = pd.to_datetime(raw_df.Date).dt.month;\nraw_df[\"day\"] = pd.to_datetime(raw_df.Date).dt.day;","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.367247Z","iopub.execute_input":"2021-12-20T11:55:30.367668Z","iopub.status.idle":"2021-12-20T11:55:30.542056Z","shell.execute_reply.started":"2021-12-20T11:55:30.367636Z","shell.execute_reply":"2021-12-20T11:55:30.541257Z"},"trusted":true},"execution_count":656,"outputs":[]},{"cell_type":"markdown","source":"So we have new columns of year, month and day in the dataset. We can now drop Date column from the dataset","metadata":{}},{"cell_type":"code","source":"raw_df.drop(columns = [\"Date\"], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.543408Z","iopub.execute_input":"2021-12-20T11:55:30.543626Z","iopub.status.idle":"2021-12-20T11:55:30.562931Z","shell.execute_reply.started":"2021-12-20T11:55:30.543599Z","shell.execute_reply":"2021-12-20T11:55:30.562088Z"},"trusted":true},"execution_count":657,"outputs":[]},{"cell_type":"code","source":"raw_df.info()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.564071Z","iopub.execute_input":"2021-12-20T11:55:30.564322Z","iopub.status.idle":"2021-12-20T11:55:30.678021Z","shell.execute_reply.started":"2021-12-20T11:55:30.564292Z","shell.execute_reply":"2021-12-20T11:55:30.677360Z"},"trusted":true},"execution_count":658,"outputs":[]},{"cell_type":"markdown","source":"We need to add these 3 columns in the numeric_cols list","metadata":{}},{"cell_type":"code","source":"#numeric_cols = raw_df.select_dtypes(include = np.number).columns.to_list()\n#print(numeric_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.679172Z","iopub.execute_input":"2021-12-20T11:55:30.679396Z","iopub.status.idle":"2021-12-20T11:55:30.682967Z","shell.execute_reply.started":"2021-12-20T11:55:30.679369Z","shell.execute_reply":"2021-12-20T11:55:30.682168Z"},"trusted":true},"execution_count":659,"outputs":[]},{"cell_type":"code","source":"#Fetching categorical or string columns from the dataset\n#categorical_cols = raw_df.select_dtypes('object').columns.to_list()\n#print(categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.684634Z","iopub.execute_input":"2021-12-20T11:55:30.684892Z","iopub.status.idle":"2021-12-20T11:55:30.696154Z","shell.execute_reply.started":"2021-12-20T11:55:30.684861Z","shell.execute_reply":"2021-12-20T11:55:30.695405Z"},"trusted":true},"execution_count":660,"outputs":[]},{"cell_type":"markdown","source":"### Splitting train, test and validation data\nWe have chronological data for years say from 2008 to 2017. We are expecting our model to predict values in the future and it is a good idea to seperate training data, validation data and test data chronologically. We will have data till 2015 as training data, validation data as data of 2015 and test data for year > 2015 ","metadata":{}},{"cell_type":"code","source":"plt.title('No. of Rows per Year')\nsns.countplot(x = year, data = raw_df);","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:30.697408Z","iopub.execute_input":"2021-12-20T11:55:30.697817Z","iopub.status.idle":"2021-12-20T11:55:31.206329Z","shell.execute_reply.started":"2021-12-20T11:55:30.697786Z","shell.execute_reply":"2021-12-20T11:55:31.205311Z"},"trusted":true},"execution_count":661,"outputs":[]},{"cell_type":"markdown","source":"Let us seperate input and target column ","metadata":{}},{"cell_type":"code","source":"train_df = raw_df[year < 2015]\nvalidate_df = raw_df[year == 2015]\ntest_df = raw_df[year > 2015]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.207585Z","iopub.execute_input":"2021-12-20T11:55:31.207832Z","iopub.status.idle":"2021-12-20T11:55:31.230038Z","shell.execute_reply.started":"2021-12-20T11:55:31.207802Z","shell.execute_reply":"2021-12-20T11:55:31.229215Z"},"trusted":true},"execution_count":662,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of train dataset\", train_df.shape)\nprint(\"Shape of validation dataset\", validate_df.shape)\nprint(\"Shape of test dataset\", test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.231336Z","iopub.execute_input":"2021-12-20T11:55:31.231645Z","iopub.status.idle":"2021-12-20T11:55:31.238328Z","shell.execute_reply.started":"2021-12-20T11:55:31.231613Z","shell.execute_reply":"2021-12-20T11:55:31.237323Z"},"trusted":true},"execution_count":663,"outputs":[]},{"cell_type":"code","source":"# Let us seperate input and target columns \n#Seperating target columns and dropping it from train, validate and test dataframes\ntrain_target = train_df[\"RainTomorrow\"]\ntrain_df.drop(columns = [\"RainTomorrow\"], inplace = True)\n\nvalidate_target = validate_df[\"RainTomorrow\"]\nvalidate_df.drop(columns = [\"RainTomorrow\"], inplace = True)\n\n\ntest_target = test_df[\"RainTomorrow\"]\ntest_df.drop(columns = [\"RainTomorrow\"], inplace = True)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.239731Z","iopub.execute_input":"2021-12-20T11:55:31.240003Z","iopub.status.idle":"2021-12-20T11:55:31.268330Z","shell.execute_reply.started":"2021-12-20T11:55:31.239973Z","shell.execute_reply":"2021-12-20T11:55:31.267008Z"},"trusted":true},"execution_count":664,"outputs":[]},{"cell_type":"code","source":"#Separating input columns\ntrain_inputs = train_df.copy()\nvalidate_inputs = validate_df.copy()\ntest_inputs = test_df.copy()","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.269591Z","iopub.execute_input":"2021-12-20T11:55:31.269819Z","iopub.status.idle":"2021-12-20T11:55:31.289579Z","shell.execute_reply.started":"2021-12-20T11:55:31.269792Z","shell.execute_reply":"2021-12-20T11:55:31.288739Z"},"trusted":true},"execution_count":665,"outputs":[]},{"cell_type":"markdown","source":"Let us print the shape of train inputs and train targets ","metadata":{}},{"cell_type":"code","source":"print(\"Shape of Train input\", train_inputs.shape)\nprint(\"Shape of Validate input\", validate_inputs.shape)\nprint(\"Shape of Test input\", test_inputs.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.294272Z","iopub.execute_input":"2021-12-20T11:55:31.294711Z","iopub.status.idle":"2021-12-20T11:55:31.302250Z","shell.execute_reply.started":"2021-12-20T11:55:31.294679Z","shell.execute_reply":"2021-12-20T11:55:31.301326Z"},"trusted":true},"execution_count":666,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of Train targer\", train_target.shape)\nprint(\"Shape of Validate target\", validate_target.shape)\nprint(\"Shape of Test target\", test_target.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.303574Z","iopub.execute_input":"2021-12-20T11:55:31.303995Z","iopub.status.idle":"2021-12-20T11:55:31.316058Z","shell.execute_reply.started":"2021-12-20T11:55:31.303963Z","shell.execute_reply":"2021-12-20T11:55:31.315038Z"},"trusted":true},"execution_count":667,"outputs":[]},{"cell_type":"markdown","source":"Get numerical and categorical columns","metadata":{}},{"cell_type":"code","source":"#Fetch numerical columns\nnumeric_cols = train_df.select_dtypes(include = np.number).columns.to_list()\nprint(\"Numerical columns\", numeric_cols)\n\n#Fetching categorical or string columns from the dataset\ncategorical_cols = train_df.select_dtypes('object').columns.to_list()\nprint(\"Categorical columns\", categorical_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.317806Z","iopub.execute_input":"2021-12-20T11:55:31.318316Z","iopub.status.idle":"2021-12-20T11:55:31.339944Z","shell.execute_reply.started":"2021-12-20T11:55:31.318279Z","shell.execute_reply":"2021-12-20T11:55:31.338848Z"},"trusted":true},"execution_count":668,"outputs":[]},{"cell_type":"markdown","source":"### Encoding categorical columns \nWe have categorical columns which must be convereted to integer as ML model cannot deal with string values. We will use OneHotEncoding as we have more than 2 classes or categories in our columns ","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder \nencoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(raw_df[categorical_cols])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.341485Z","iopub.execute_input":"2021-12-20T11:55:31.341770Z","iopub.status.idle":"2021-12-20T11:55:31.425200Z","shell.execute_reply.started":"2021-12-20T11:55:31.341739Z","shell.execute_reply":"2021-12-20T11:55:31.424206Z"},"trusted":true},"execution_count":669,"outputs":[]},{"cell_type":"code","source":"#Fetch new columns which will be created as a part of OneHotEncoding operation\nencoded_cols = list(encoder.get_feature_names(categorical_cols))\nprint(encoded_cols)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.426682Z","iopub.execute_input":"2021-12-20T11:55:31.427040Z","iopub.status.idle":"2021-12-20T11:55:31.434030Z","shell.execute_reply.started":"2021-12-20T11:55:31.426996Z","shell.execute_reply":"2021-12-20T11:55:31.433013Z"},"trusted":true},"execution_count":670,"outputs":[]},{"cell_type":"code","source":"#Transform categorical cols for train, test and validation dataset\ntrain_inputs[encoded_cols] = encoder.transform(train_inputs[categorical_cols])\nvalidate_inputs[encoded_cols] = encoder.transform(validate_inputs[categorical_cols])\ntest_inputs[encoded_cols] = encoder.transform(test_inputs[categorical_cols])","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:31.435449Z","iopub.execute_input":"2021-12-20T11:55:31.435760Z","iopub.status.idle":"2021-12-20T11:55:32.219472Z","shell.execute_reply.started":"2021-12-20T11:55:31.435720Z","shell.execute_reply":"2021-12-20T11:55:32.218269Z"},"trusted":true},"execution_count":671,"outputs":[]},{"cell_type":"code","source":"print(\"Shape of training dataset\", train_inputs.shape)\nprint(\"Shape of validation dataset\", validate_inputs.shape)\nprint(\"Shape of test dataset\", test_inputs.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:32.220952Z","iopub.execute_input":"2021-12-20T11:55:32.221314Z","iopub.status.idle":"2021-12-20T11:55:32.227869Z","shell.execute_reply.started":"2021-12-20T11:55:32.221233Z","shell.execute_reply":"2021-12-20T11:55:32.226970Z"},"trusted":true},"execution_count":672,"outputs":[]},{"cell_type":"code","source":"X_train = train_inputs[numeric_cols + encoded_cols]\nX_validate = validate_inputs[numeric_cols + encoded_cols]\nX_test = test_inputs[numeric_cols + encoded_cols]","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:32.229452Z","iopub.execute_input":"2021-12-20T11:55:32.229948Z","iopub.status.idle":"2021-12-20T11:55:32.371615Z","shell.execute_reply.started":"2021-12-20T11:55:32.229905Z","shell.execute_reply":"2021-12-20T11:55:32.370664Z"},"trusted":true},"execution_count":673,"outputs":[]},{"cell_type":"markdown","source":"# Training model using DecisionTreeClassifier","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:32.373133Z","iopub.execute_input":"2021-12-20T11:55:32.373521Z","iopub.status.idle":"2021-12-20T11:55:32.378951Z","shell.execute_reply.started":"2021-12-20T11:55:32.373476Z","shell.execute_reply":"2021-12-20T11:55:32.378002Z"},"trusted":true},"execution_count":674,"outputs":[]},{"cell_type":"code","source":"%%time\ndt.fit(X_train, train_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:32.380221Z","iopub.execute_input":"2021-12-20T11:55:32.380461Z","iopub.status.idle":"2021-12-20T11:55:36.172998Z","shell.execute_reply.started":"2021-12-20T11:55:32.380431Z","shell.execute_reply":"2021-12-20T11:55:36.171807Z"},"trusted":true},"execution_count":675,"outputs":[]},{"cell_type":"markdown","source":"# Model evaluation and metrics for train, test and validation ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:36.174687Z","iopub.execute_input":"2021-12-20T11:55:36.175053Z","iopub.status.idle":"2021-12-20T11:55:36.179845Z","shell.execute_reply.started":"2021-12-20T11:55:36.175016Z","shell.execute_reply":"2021-12-20T11:55:36.178929Z"},"trusted":true},"execution_count":676,"outputs":[]},{"cell_type":"markdown","source":"## Predict and evaluate model performance on training data itself","metadata":{}},{"cell_type":"code","source":"train_preds = dt.predict(X_train)\nprint(pd.value_counts(train_preds))","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:36.181297Z","iopub.execute_input":"2021-12-20T11:55:36.182175Z","iopub.status.idle":"2021-12-20T11:55:36.303656Z","shell.execute_reply.started":"2021-12-20T11:55:36.182096Z","shell.execute_reply":"2021-12-20T11:55:36.302691Z"},"trusted":true},"execution_count":677,"outputs":[]},{"cell_type":"markdown","source":"Let us check the accuracy score -","metadata":{}},{"cell_type":"code","source":"accuracy_score(train_target, train_preds)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:36.305042Z","iopub.execute_input":"2021-12-20T11:55:36.306004Z","iopub.status.idle":"2021-12-20T11:55:36.762855Z","shell.execute_reply.started":"2021-12-20T11:55:36.305955Z","shell.execute_reply":"2021-12-20T11:55:36.761930Z"},"trusted":true},"execution_count":678,"outputs":[]},{"cell_type":"markdown","source":"As we can see above accuracy score is 1.0 which is possibly correct as our model was trained on the same data which was used for prediction. Let us try to get the score of the model for validation dataet ","metadata":{}},{"cell_type":"code","source":"dt.score(X_validate, validate_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:36.763851Z","iopub.execute_input":"2021-12-20T11:55:36.764044Z","iopub.status.idle":"2021-12-20T11:55:36.856162Z","shell.execute_reply.started":"2021-12-20T11:55:36.764019Z","shell.execute_reply":"2021-12-20T11:55:36.855069Z"},"trusted":true},"execution_count":679,"outputs":[]},{"cell_type":"code","source":"dt.score(X_test, test_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:36.857335Z","iopub.execute_input":"2021-12-20T11:55:36.857604Z","iopub.status.idle":"2021-12-20T11:55:36.992755Z","shell.execute_reply.started":"2021-12-20T11:55:36.857571Z","shell.execute_reply":"2021-12-20T11:55:36.991774Z"},"trusted":true},"execution_count":680,"outputs":[]},{"cell_type":"markdown","source":"# Drawing the decision tree ","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import plot_tree, export_text\nplt.figure(figsize=(80,20))\nplot_tree(dt, feature_names=X_train.columns, max_depth=2, filled=True);","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:55:36.994308Z","iopub.execute_input":"2021-12-20T11:55:36.994546Z","iopub.status.idle":"2021-12-20T11:55:38.491442Z","shell.execute_reply.started":"2021-12-20T11:55:36.994511Z","shell.execute_reply":"2021-12-20T11:55:38.490463Z"},"trusted":true},"execution_count":681,"outputs":[]},{"cell_type":"markdown","source":"Let us get the max depth of the tree","metadata":{}},{"cell_type":"code","source":"#Max depth of decision tree\nprint(\"Depth of the decision tree\", dt.tree_.max_depth)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:56:48.010901Z","iopub.execute_input":"2021-12-20T11:56:48.011652Z","iopub.status.idle":"2021-12-20T11:56:48.016547Z","shell.execute_reply.started":"2021-12-20T11:56:48.011611Z","shell.execute_reply":"2021-12-20T11:56:48.015908Z"},"trusted":true},"execution_count":687,"outputs":[]},{"cell_type":"markdown","source":"We can also get the feature importance which will give us a clue on which feature is the most important feature impacting the decision","metadata":{}},{"cell_type":"code","source":"print(dt.feature_importances_)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:56:09.815925Z","iopub.execute_input":"2021-12-20T11:56:09.816237Z","iopub.status.idle":"2021-12-20T11:56:09.823842Z","shell.execute_reply.started":"2021-12-20T11:56:09.816202Z","shell.execute_reply":"2021-12-20T11:56:09.822934Z"},"trusted":true},"execution_count":685,"outputs":[]},{"cell_type":"code","source":"\nimportance_df = pd.DataFrame({\n    'feature': X_train.columns,\n    'importance': dt.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(importance_df.head())","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:57:57.406587Z","iopub.execute_input":"2021-12-20T11:57:57.407291Z","iopub.status.idle":"2021-12-20T11:57:57.418269Z","shell.execute_reply.started":"2021-12-20T11:57:57.407236Z","shell.execute_reply":"2021-12-20T11:57:57.417388Z"},"trusted":true},"execution_count":689,"outputs":[]},{"cell_type":"code","source":"plt.title('Feature Importance')\nsns.barplot(data = importance_df.head(10), x='importance', y='feature');","metadata":{"execution":{"iopub.status.busy":"2021-12-20T11:58:13.060024Z","iopub.execute_input":"2021-12-20T11:58:13.060625Z","iopub.status.idle":"2021-12-20T11:58:13.419274Z","shell.execute_reply.started":"2021-12-20T11:58:13.060574Z","shell.execute_reply":"2021-12-20T11:58:13.418369Z"},"trusted":true},"execution_count":690,"outputs":[]},{"cell_type":"markdown","source":"# Hyperparamter tuning \nOur model had 100 % training accuracy which means that model is memorising the inputs. Comparing it with validation and test accuracy of approx. 77 % we clearly see a case of overfitting. We need to try and make some changes in the paramters of model training to avoid overfitting. One possible way of doing it is to reduce the max depth of the tree. Let us train the model again","metadata":{}},{"cell_type":"code","source":"dt = DecisionTreeClassifier(max_depth = 4, random_state = 42)\ndt.fit(X_train, train_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:02:35.370105Z","iopub.execute_input":"2021-12-20T12:02:35.370418Z","iopub.status.idle":"2021-12-20T12:02:36.301945Z","shell.execute_reply.started":"2021-12-20T12:02:35.370387Z","shell.execute_reply":"2021-12-20T12:02:36.300903Z"},"trusted":true},"execution_count":693,"outputs":[]},{"cell_type":"markdown","source":"Let us score the model on training, validation and test dataset again","metadata":{}},{"cell_type":"code","source":"#Scoring against training dataset\ndt.score(X_train, train_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:03:31.008214Z","iopub.execute_input":"2021-12-20T12:03:31.008521Z","iopub.status.idle":"2021-12-20T12:03:31.535827Z","shell.execute_reply.started":"2021-12-20T12:03:31.008488Z","shell.execute_reply":"2021-12-20T12:03:31.534983Z"},"trusted":true},"execution_count":695,"outputs":[]},{"cell_type":"markdown","source":"As we can see the training accuracy is just 83% which means the model is not memorising and overfitting the values. Let us try the same for validation and test dataset","metadata":{}},{"cell_type":"code","source":"#Scoring against validation dataset\ndt.score(X_validate, validate_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:04:47.242002Z","iopub.execute_input":"2021-12-20T12:04:47.242312Z","iopub.status.idle":"2021-12-20T12:04:47.324273Z","shell.execute_reply.started":"2021-12-20T12:04:47.242277Z","shell.execute_reply":"2021-12-20T12:04:47.323304Z"},"trusted":true},"execution_count":696,"outputs":[]},{"cell_type":"code","source":"#scoring against test dataset\ndt.score(X_test, test_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:05:16.926374Z","iopub.execute_input":"2021-12-20T12:05:16.926639Z","iopub.status.idle":"2021-12-20T12:05:17.049602Z","shell.execute_reply.started":"2021-12-20T12:05:16.926610Z","shell.execute_reply":"2021-12-20T12:05:17.048598Z"},"trusted":true},"execution_count":697,"outputs":[]},{"cell_type":"markdown","source":"We now have a significantly better performance on training and test dataset Let us get the confusion matrix ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:06:40.815293Z","iopub.execute_input":"2021-12-20T12:06:40.815598Z","iopub.status.idle":"2021-12-20T12:06:40.820300Z","shell.execute_reply.started":"2021-12-20T12:06:40.815568Z","shell.execute_reply":"2021-12-20T12:06:40.819211Z"},"trusted":true},"execution_count":698,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix for training data\ntrain_pred = dt.predict(X_train)\nmatrix_train = confusion_matrix(train_target, train_pred)\nprint(matrix_train)\n\nmatrix_train = classification_report(train_target, train_pred)\nprint(matrix_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:10:52.695789Z","iopub.execute_input":"2021-12-20T12:10:52.696180Z","iopub.status.idle":"2021-12-20T12:11:01.741072Z","shell.execute_reply.started":"2021-12-20T12:10:52.696140Z","shell.execute_reply":"2021-12-20T12:11:01.740008Z"},"trusted":true},"execution_count":705,"outputs":[]},{"cell_type":"code","source":"#Confusion matrix for validation data\nvalidation_pred = dt.predict(X_validate)\nmatrix_validate = confusion_matrix(validate_target, validation_pred)\nprint(matrix_validate)\n\nmatrix_validate = classification_report(validate_target, validation_pred)\nprint(matrix_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:11:50.326081Z","iopub.execute_input":"2021-12-20T12:11:50.326388Z","iopub.status.idle":"2021-12-20T12:11:51.640470Z","shell.execute_reply.started":"2021-12-20T12:11:50.326357Z","shell.execute_reply":"2021-12-20T12:11:51.639396Z"},"trusted":true},"execution_count":706,"outputs":[]},{"cell_type":"code","source":"test_pred = dt.predict(X_test)\nmatrix_test = confusion_matrix(test_target, test_pred)\nprint(matrix_test)\n\nmatrix_test = classification_report(test_target, test_pred)\nprint(matrix_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:12:17.570824Z","iopub.execute_input":"2021-12-20T12:12:17.571437Z","iopub.status.idle":"2021-12-20T12:12:19.687335Z","shell.execute_reply.started":"2021-12-20T12:12:17.571391Z","shell.execute_reply":"2021-12-20T12:12:19.686446Z"},"trusted":true},"execution_count":707,"outputs":[]},{"cell_type":"markdown","source":"# Train Random Forest algorithm\nRamdom Forest is an ensemble technique where\n* multiple DecisionTrees will be trained with different hyperparatmers\n* outcome of each DecisionTree will be voted / averaged \n* the one with most count in terms of Classifier will be the winner prediction","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_jobs = 1, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:17:57.336561Z","iopub.execute_input":"2021-12-20T12:17:57.338211Z","iopub.status.idle":"2021-12-20T12:17:57.343832Z","shell.execute_reply.started":"2021-12-20T12:17:57.338056Z","shell.execute_reply":"2021-12-20T12:17:57.342665Z"},"trusted":true},"execution_count":709,"outputs":[]},{"cell_type":"code","source":"%%time\nrfc.fit(X_train, train_target)","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:18:21.965672Z","iopub.execute_input":"2021-12-20T12:18:21.966707Z","iopub.status.idle":"2021-12-20T12:18:51.396466Z","shell.execute_reply.started":"2021-12-20T12:18:21.966651Z","shell.execute_reply":"2021-12-20T12:18:51.395529Z"},"trusted":true},"execution_count":710,"outputs":[]},{"cell_type":"markdown","source":"Let us now get the score of model for train, test and validation dataset","metadata":{}},{"cell_type":"code","source":"print(\"Training accuracy = \", rfc.score(X_train, train_target) * 100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:21:57.771431Z","iopub.execute_input":"2021-12-20T12:21:57.771708Z","iopub.status.idle":"2021-12-20T12:22:01.641684Z","shell.execute_reply.started":"2021-12-20T12:21:57.771680Z","shell.execute_reply":"2021-12-20T12:22:01.640670Z"},"trusted":true},"execution_count":713,"outputs":[]},{"cell_type":"code","source":"print(\"Validation accuracy = \", rfc.score(X_validate, validate_target) * 100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:22:33.125382Z","iopub.execute_input":"2021-12-20T12:22:33.125649Z","iopub.status.idle":"2021-12-20T12:22:33.887868Z","shell.execute_reply.started":"2021-12-20T12:22:33.125622Z","shell.execute_reply":"2021-12-20T12:22:33.886853Z"},"trusted":true},"execution_count":714,"outputs":[]},{"cell_type":"code","source":"print(\"Test accuracy = \", rfc.score(X_test, test_target) * 100, \"%\")","metadata":{"execution":{"iopub.status.busy":"2021-12-20T12:24:22.206306Z","iopub.execute_input":"2021-12-20T12:24:22.206621Z","iopub.status.idle":"2021-12-20T12:24:23.314144Z","shell.execute_reply.started":"2021-12-20T12:24:22.206575Z","shell.execute_reply":"2021-12-20T12:24:23.312935Z"},"trusted":true},"execution_count":715,"outputs":[]}]}